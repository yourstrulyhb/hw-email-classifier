{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b99b874",
   "metadata": {},
   "source": [
    "<h1>Machine Learning - Lecture 6 Homework</h1>\n",
    "<h3>Hannah Bella C. Arceño</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b202e6",
   "metadata": {},
   "source": [
    "### Basic modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "858933ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51665af6",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95ef53c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filepath</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../data/000/000</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../data/000/001</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../data/000/002</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../data/000/003</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../data/000/004</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          filepath label\n",
       "0  ../data/000/000   ham\n",
       "1  ../data/000/001  spam\n",
       "2  ../data/000/002  spam\n",
       "3  ../data/000/003   ham\n",
       "4  ../data/000/004  spam"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABELS_FILE = \"../trec06p-cs280/labels\"\n",
    "\n",
    "# Get labels\n",
    "labels_stream = open(LABELS_FILE, 'r', encoding='utf-8')\n",
    "labels = labels_stream.read()\n",
    "labels_stream.close()\n",
    "\n",
    "x = re.split(\"\\s\", labels)\n",
    "\n",
    "# Create dataframe for labels-filepath\n",
    "labels = []\n",
    "filepaths = []\n",
    "\n",
    "for i in range(len(x)):\n",
    "    if i%2 == 0:\n",
    "        labels.append(x[i])\n",
    "    else:\n",
    "        filepaths.append(x[i])\n",
    "        \n",
    "labels.pop() # Remove last blank line\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df.insert(0, \"filepath\", filepaths, True)\n",
    "df.insert(1, \"label\", labels, True)\n",
    "\n",
    "# Split training and testing dataset filepaths\n",
    "split_train_and_test_at = filepaths.index(\"../data/071/000\")\n",
    "\n",
    "train_set = df.iloc[:split_train_and_test_at]\n",
    "test_set = df.iloc[split_train_and_test_at:]\n",
    "             \n",
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "604243f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21300, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20b33297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16522, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5b6084",
   "metadata": {},
   "source": [
    "## Get essential email words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c84037",
   "metadata": {},
   "source": [
    "### Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73f02bee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['actually', 'added', 'adj', 'adopted', 'affected']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract email body, or words for train_ham and train_spam\n",
    "# Remove alphanumeric characters and punctuation marks, and stop words\n",
    "\n",
    "# Stop words\n",
    "stop_words_stream = open(\"../stop_words.txt\", 'r', encoding='utf-8')\n",
    "stop_words_string = stop_words_stream.read()\n",
    "stop_words_stream.close()\n",
    "\n",
    "STOP_WORDS_LIST = re.split(\"\\s\", stop_words_string)\n",
    "STOP_WORDS_LIST.pop() # Remove blank line\n",
    "\n",
    "STOP_WORDS_LIST[10:15]                       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cd9bae",
   "metadata": {},
   "source": [
    "### NLTK - for natural language processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "132f3cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('words')\n",
    "\n",
    "nltk_words = set(nltk.corpus.words.words())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0e1a88",
   "metadata": {},
   "source": [
    "### Extracting email body and essential words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1789b866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Based on: https://stackoverflow.com/questions/54760414/get-only-body-of-letters-emails-from-text-files\n",
    "# from email import message_from_binary_file\n",
    "\n",
    "# parent_path = \"trec06p-cs280/data/\"\n",
    "# filepaths = train_set['filepath'].tolist() # List of filepaths for ham emails (training set)\n",
    "\n",
    "# messages = [] # List containing essential words in email message/body\n",
    "\n",
    "# # Open and read every email in train set\n",
    "# for filepath in filepaths:\n",
    "#     rel_path =  parent_path + filepath\n",
    "\n",
    "#     with open(rel_path, 'rb') as file:\n",
    "#         email_file = message_from_binary_file(file)\n",
    "#         email_body = \"\"\n",
    "\n",
    "#         if email_file.is_multipart():\n",
    "#             # Take first text/plain part as email body\n",
    "#             for part in email_file.walk():\n",
    "#                 if part.get_content_type() == 'text/plain':\n",
    "#                     email_body = part.get_payload().lower()\n",
    "#                     break\n",
    "#         else:\n",
    "#             email_body = email_file.get_payload().lower()\n",
    "\n",
    "#         # Remove unnecessary elements\n",
    "#         email_body = re.sub(r'[A-Za-z0-9]*@[A-Za-z]*\\.?[A-Za-z0-9]*', \"\", email_body) # Remove email addresses\n",
    "#         email_body = re.sub(r'[0-9]+', \"\", email_body) # Remove numbers\n",
    "#         email_body = re.sub(r'[^\\'\\w\\s]', \"\", email_body) #  Remove all punctuation marks except apostrophe (')\n",
    "\n",
    "#         words_list = re.split(\"\\s\", email_body)\n",
    "\n",
    "#         while(\"\" in words_list): # Remove whitespaces\n",
    "#             words_list.remove(\"\")\n",
    "\n",
    "#         # Remove stop words\n",
    "#         essential_words_list = [word for word in words_list if word not in STOP_WORDS_LIST]\n",
    "        \n",
    "#         # Use NLTK to further remove unnecessary words/characters\n",
    "#         essential_words_list = [word for word in essential_words_list if word in nltk_words]\n",
    "\n",
    "#         message = \" \".join(essential_words_list)\n",
    "\n",
    "#         messages.append(message)\n",
    "\n",
    "\n",
    "# train_set.insert(2, \"message\", messages, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22bf6822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save data to csv file (so that we don't need to run email extraction again for train set (takes 10-20 mins.))\n",
    "# train_set.to_csv('train_set.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c222483a",
   "metadata": {},
   "source": [
    "## Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6db2be78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>filepath</th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>../data/000/000</td>\n",
       "      <td>ham</td>\n",
       "      <td>list ago running set archive server official l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>../data/000/001</td>\n",
       "      <td>spam</td>\n",
       "      <td>luxury buy frank muller omega tag full gold me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>../data/000/002</td>\n",
       "      <td>spam</td>\n",
       "      <td>academic prestigious knowledge experience lack...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>../data/000/003</td>\n",
       "      <td>ham</td>\n",
       "      <td>verify subscription list charter day order chu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>../data/000/004</td>\n",
       "      <td>spam</td>\n",
       "      <td>luscious continued tonsillitis</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0         filepath label  \\\n",
       "0           0  ../data/000/000   ham   \n",
       "1           1  ../data/000/001  spam   \n",
       "2           2  ../data/000/002  spam   \n",
       "3           3  ../data/000/003   ham   \n",
       "4           4  ../data/000/004  spam   \n",
       "\n",
       "                                             message  \n",
       "0  list ago running set archive server official l...  \n",
       "1  luxury buy frank muller omega tag full gold me...  \n",
       "2  academic prestigious knowledge experience lack...  \n",
       "3  verify subscription list charter day order chu...  \n",
       "4                     luscious continued tonsillitis  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train Set\n",
    "\n",
    "train_set = pd.read_csv('train_set.csv')\n",
    "train_set = train_set.fillna(\"\")\n",
    "train_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6cdacb",
   "metadata": {},
   "source": [
    "## Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d867353f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "top_words = Counter(\" \".join(train_set['message']).split()).most_common(1000)\n",
    "   \n",
    "VOCABULARY = [x[0] for x in top_words]\n",
    "len(VOCABULARY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8ddd79e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('will', 11321),\n",
       " ('width', 5239),\n",
       " ('board', 5164),\n",
       " ('company', 4403),\n",
       " ('price', 4391)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c62fd890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['will',\n",
       " 'width',\n",
       " 'board',\n",
       " 'company',\n",
       " 'price',\n",
       " 'gold',\n",
       " 'list',\n",
       " 'nil',\n",
       " 'time',\n",
       " 'help',\n",
       " 'send',\n",
       " 'message',\n",
       " 'subject',\n",
       " 'adobe',\n",
       " 'size',\n",
       " 'body',\n",
       " 'received',\n",
       " 'program',\n",
       " 'work',\n",
       " 'wrote',\n",
       " 'professional',\n",
       " 'good',\n",
       " 'number',\n",
       " 'border',\n",
       " 'university',\n",
       " 'well',\n",
       " 'problem',\n",
       " 'table',\n",
       " 'stock',\n",
       " 'file',\n",
       " 'bit',\n",
       " 'color',\n",
       " 'font',\n",
       " 'de',\n",
       " 'office',\n",
       " 'add',\n",
       " 'current',\n",
       " 'news',\n",
       " 'code',\n",
       " 'development',\n",
       " 'corp',\n",
       " 'pro',\n",
       " 'find',\n",
       " 'china',\n",
       " 'head',\n",
       " 'great',\n",
       " 'read',\n",
       " 'system',\n",
       " 'today',\n",
       " 'best',\n",
       " 'people',\n",
       " 'power',\n",
       " 'call',\n",
       " 'motor',\n",
       " 'save',\n",
       " 'text',\n",
       " 'handy',\n",
       " 'data',\n",
       " 'address',\n",
       " 'height',\n",
       " 'mail',\n",
       " 'set',\n",
       " 'days',\n",
       " 'site',\n",
       " 'market',\n",
       " 'free',\n",
       " 'div',\n",
       " 'web',\n",
       " 'better',\n",
       " 'life',\n",
       " 'additional',\n",
       " 'oil',\n",
       " 'meta',\n",
       " 'big',\n",
       " 'port',\n",
       " 'control',\n",
       " 'offer',\n",
       " 'money',\n",
       " 'day',\n",
       " 'computer',\n",
       " 'campaign',\n",
       " 'rating',\n",
       " 'gas',\n",
       " 'retail',\n",
       " 'forward',\n",
       " 'cart',\n",
       " 'robot',\n",
       " 'technology',\n",
       " 'experience',\n",
       " 'service',\n",
       " 'order',\n",
       " 'week',\n",
       " 'ra',\n",
       " 'energy',\n",
       " 'version',\n",
       " 'potential',\n",
       " 'project',\n",
       " 'based',\n",
       " 'exploration',\n",
       " 'going',\n",
       " 'long',\n",
       " 'real',\n",
       " 'special',\n",
       " 'full',\n",
       " 'provide',\n",
       " 'post',\n",
       " 'high',\n",
       " 'start',\n",
       " 'report',\n",
       " 'thing',\n",
       " 'sender',\n",
       " 'international',\n",
       " 'question',\n",
       " 'business',\n",
       " 'works',\n",
       " 'small',\n",
       " 'support',\n",
       " 'digital',\n",
       " 'working',\n",
       " 'serial',\n",
       " 'check',\n",
       " 'yew',\n",
       " 'space',\n",
       " 'tue',\n",
       " 'mon',\n",
       " 'expansion',\n",
       " 'input',\n",
       " 'style',\n",
       " 'currently',\n",
       " 'department',\n",
       " 'original',\n",
       " 'account',\n",
       " 'contact',\n",
       " 'science',\n",
       " 'point',\n",
       " 'plan',\n",
       " 'video',\n",
       " 'span',\n",
       " 'interested',\n",
       " 'receive',\n",
       " 'change',\n",
       " 'second',\n",
       " 'running',\n",
       " 'design',\n",
       " 'apple',\n",
       " 'error',\n",
       " 'short',\n",
       " 'output',\n",
       " 'place',\n",
       " 'side',\n",
       " 'year',\n",
       " 'low',\n",
       " 'sensor',\n",
       " 'access',\n",
       " 'light',\n",
       " 'golden',\n",
       " 'times',\n",
       " 'dear',\n",
       " 'release',\n",
       " 'phone',\n",
       " 'option',\n",
       " 'crust',\n",
       " 'battery',\n",
       " 'note',\n",
       " 'hope',\n",
       " 'group',\n",
       " 'course',\n",
       " 'three',\n",
       " 'fine',\n",
       " 'form',\n",
       " 'test',\n",
       " 'fast',\n",
       " 'application',\n",
       " 'hello',\n",
       " 'include',\n",
       " 'st',\n",
       " 'product',\n",
       " 'plain',\n",
       " 'turn',\n",
       " 'kind',\n",
       " 'servo',\n",
       " 'watch',\n",
       " 'case',\n",
       " 'format',\n",
       " 'standard',\n",
       " 'future',\n",
       " 'idea',\n",
       " 'pin',\n",
       " 'memory',\n",
       " 'chip',\n",
       " 'pine',\n",
       " 'mode',\n",
       " 'common',\n",
       " 'corporation',\n",
       " 'write',\n",
       " 'sonar',\n",
       " 'starting',\n",
       " 'limited',\n",
       " 'effects',\n",
       " 'committee',\n",
       " 'class',\n",
       " 'voltage',\n",
       " 'complete',\n",
       " 'store',\n",
       " 'buy',\n",
       " 'lot',\n",
       " 'prospect',\n",
       " 'press',\n",
       " 'process',\n",
       " 'sun',\n",
       " 'example',\n",
       " 'production',\n",
       " 'share',\n",
       " 'type',\n",
       " 'agreement',\n",
       " 'simply',\n",
       " 'joint',\n",
       " 'center',\n",
       " 'longer',\n",
       " 'large',\n",
       " 'seismic',\n",
       " 'link',\n",
       " 'visit',\n",
       " 'area',\n",
       " 'book',\n",
       " 'ago',\n",
       " 'mar',\n",
       " 'field',\n",
       " 'answer',\n",
       " 'source',\n",
       " 'private',\n",
       " 'issue',\n",
       " 'feel',\n",
       " 'investment',\n",
       " 'allow',\n",
       " 'delay',\n",
       " 'opportunity',\n",
       " 'launch',\n",
       " 'click',\n",
       " 'word',\n",
       " 'update',\n",
       " 'simple',\n",
       " 'engineering',\n",
       " 'building',\n",
       " 'shipping',\n",
       " 'drive',\n",
       " 'advance',\n",
       " 'san',\n",
       " 'main',\n",
       " 'record',\n",
       " 'solution',\n",
       " 'general',\n",
       " 'matter',\n",
       " 'sincerely',\n",
       " 'third',\n",
       " 'shareholder',\n",
       " 'effective',\n",
       " 'student',\n",
       " 'fact',\n",
       " 'return',\n",
       " 'thought',\n",
       " 'connected',\n",
       " 'wrong',\n",
       " 'discussion',\n",
       " 'natural',\n",
       " 'left',\n",
       " 'school',\n",
       " 'canyon',\n",
       " 'true',\n",
       " 'interest',\n",
       " 'men',\n",
       " 'north',\n",
       " 'trade',\n",
       " 'collection',\n",
       " 'easy',\n",
       " 'mineral',\n",
       " 'hard',\n",
       " 'led',\n",
       " 'function',\n",
       " 'extension',\n",
       " 'directly',\n",
       " 'connect',\n",
       " 'build',\n",
       " 'history',\n",
       " 'edition',\n",
       " 'management',\n",
       " 'reading',\n",
       " 'signal',\n",
       " 'acrobat',\n",
       " 'swath',\n",
       " 'device',\n",
       " 'display',\n",
       " 'mining',\n",
       " 'conference',\n",
       " 'weight',\n",
       " 'split',\n",
       " 'public',\n",
       " 'character',\n",
       " 'top',\n",
       " 'deal',\n",
       " 'interface',\n",
       " 'engaged',\n",
       " 'normal',\n",
       " 'independent',\n",
       " 'increase',\n",
       " 'machine',\n",
       " 'degree',\n",
       " 'box',\n",
       " 'speed',\n",
       " 'huge',\n",
       " 'choice',\n",
       " 'position',\n",
       " 'expect',\n",
       " 'manager',\n",
       " 'premiere',\n",
       " 'fully',\n",
       " 'reply',\n",
       " 'pretty',\n",
       " 'circuit',\n",
       " 'close',\n",
       " 'play',\n",
       " 'library',\n",
       " 'west',\n",
       " 'sell',\n",
       " 'making',\n",
       " 'lose',\n",
       " 'president',\n",
       " 'move',\n",
       " 'mike',\n",
       " 'remember',\n",
       " 'open',\n",
       " 'credit',\n",
       " 'quality',\n",
       " 'total',\n",
       " 'response',\n",
       " 'paper',\n",
       " 'meeting',\n",
       " 'june',\n",
       " 'language',\n",
       " 'turned',\n",
       " 'te',\n",
       " 'sharp',\n",
       " 'server',\n",
       " 'local',\n",
       " 'producer',\n",
       " 'cost',\n",
       " 'luck',\n",
       " 'correct',\n",
       " 'la',\n",
       " 'communication',\n",
       " 'supply',\n",
       " 'range',\n",
       " 'load',\n",
       " 'water',\n",
       " 'result',\n",
       " 'college',\n",
       " 'island',\n",
       " 'major',\n",
       " 'loss',\n",
       " 'reference',\n",
       " 'early',\n",
       " 'customer',\n",
       " 'network',\n",
       " 'mime',\n",
       " 'country',\n",
       " 'advice',\n",
       " 'pa',\n",
       " 'consider',\n",
       " 'request',\n",
       " 'driver',\n",
       " 'risk',\n",
       " 'cash',\n",
       " 'copy',\n",
       " 'venture',\n",
       " 'focus',\n",
       " 'level',\n",
       " 'highly',\n",
       " 'happy',\n",
       " 'marketing',\n",
       " 'interesting',\n",
       " 'ability',\n",
       " 'pay',\n",
       " 'sat',\n",
       " 'multiple',\n",
       " 'south',\n",
       " 'user',\n",
       " 'net',\n",
       " 'strong',\n",
       " 'director',\n",
       " 'media',\n",
       " 'tenure',\n",
       " 'industry',\n",
       " 'stuff',\n",
       " 'understand',\n",
       " 'wondering',\n",
       " 'sector',\n",
       " 'growth',\n",
       " 'knowledge',\n",
       " 'ascii',\n",
       " 'soft',\n",
       " 'coming',\n",
       " 'august',\n",
       " 'bad',\n",
       " 'drilling',\n",
       " 'written',\n",
       " 'ross',\n",
       " 'encore',\n",
       " 'month',\n",
       " 'unit',\n",
       " 'interactive',\n",
       " 'loan',\n",
       " 'card',\n",
       " 'screen',\n",
       " 'key',\n",
       " 'audition',\n",
       " 'bank',\n",
       " 'series',\n",
       " 'person',\n",
       " 'command',\n",
       " 'hardware',\n",
       " 'friend',\n",
       " 'volume',\n",
       " 'job',\n",
       " 'term',\n",
       " 'talk',\n",
       " 'exciting',\n",
       " 'love',\n",
       " 'martin',\n",
       " 'green',\n",
       " 'delivery',\n",
       " 'aa',\n",
       " 'institute',\n",
       " 'rate',\n",
       " 'trace',\n",
       " 'unique',\n",
       " 'room',\n",
       " 'difficult',\n",
       " 'national',\n",
       " 'sex',\n",
       " 'team',\n",
       " 'franklin',\n",
       " 'global',\n",
       " 'manual',\n",
       " 'formula',\n",
       " 'charge',\n",
       " 'couple',\n",
       " 'continue',\n",
       " 'trading',\n",
       " 'create',\n",
       " 'man',\n",
       " 'graduate',\n",
       " 'search',\n",
       " 'single',\n",
       " 'cable',\n",
       " 'register',\n",
       " 'higher',\n",
       " 'geological',\n",
       " 'exactly',\n",
       " 'health',\n",
       " 'rich',\n",
       " 'enjoy',\n",
       " 'red',\n",
       " 'ready',\n",
       " 'writing',\n",
       " 'inform',\n",
       " 'march',\n",
       " 'reason',\n",
       " 'remove',\n",
       " 'bulk',\n",
       " 'frank',\n",
       " 'licensed',\n",
       " 'dont',\n",
       " 'item',\n",
       " 'nice',\n",
       " 'initial',\n",
       " 'sound',\n",
       " 'included',\n",
       " 'letter',\n",
       " 'en',\n",
       " 'hot',\n",
       " 'confirm',\n",
       " 'ground',\n",
       " 'purchase',\n",
       " 'chips',\n",
       " 'tomorrow',\n",
       " 'successful',\n",
       " 'worked',\n",
       " 'species',\n",
       " 'switch',\n",
       " 'professor',\n",
       " 'house',\n",
       " 'designed',\n",
       " 'winning',\n",
       " 'personal',\n",
       " 'eye',\n",
       " 'inside',\n",
       " 'mobile',\n",
       " 'funds',\n",
       " 'climb',\n",
       " 'security',\n",
       " 'break',\n",
       " 'connection',\n",
       " 'lottery',\n",
       " 'foreign',\n",
       " 'advanced',\n",
       " 'radio',\n",
       " 'amount',\n",
       " 'void',\n",
       " 'exchange',\n",
       " 'specific',\n",
       " 'geophysical',\n",
       " 'chance',\n",
       " 'base',\n",
       " 'precedence',\n",
       " 'care',\n",
       " 'extra',\n",
       " 'directory',\n",
       " 'generate',\n",
       " 'resource',\n",
       " 'face',\n",
       " 'resistor',\n",
       " 'profile',\n",
       " 'method',\n",
       " 'engine',\n",
       " 'moving',\n",
       " 'registration',\n",
       " 'model',\n",
       " 'built',\n",
       " 'phase',\n",
       " 'statement',\n",
       " 'las',\n",
       " 'ma',\n",
       " 'electronics',\n",
       " 'secure',\n",
       " 'appreciate',\n",
       " 'technical',\n",
       " 'aggressive',\n",
       " 'realize',\n",
       " 'visa',\n",
       " 'material',\n",
       " 'ram',\n",
       " 'assembly',\n",
       " 'confidentiality',\n",
       " 'respect',\n",
       " 'guess',\n",
       " 'wide',\n",
       " 'reset',\n",
       " 'pulse',\n",
       " 'legal',\n",
       " 'draw',\n",
       " 'category',\n",
       " 'article',\n",
       " 'finally',\n",
       " 'avoid',\n",
       " 'voice',\n",
       " 'meaning',\n",
       " 'setting',\n",
       " 'bilbo',\n",
       " 'operating',\n",
       " 'stepper',\n",
       " 'black',\n",
       " 'pick',\n",
       " 'claim',\n",
       " 'distance',\n",
       " 'respond',\n",
       " 'mind',\n",
       " 'advantage',\n",
       " 'study',\n",
       " 'analysis',\n",
       " 'tremendous',\n",
       " 'providence',\n",
       " 'community',\n",
       " 'rat',\n",
       " 'sort',\n",
       " 'clear',\n",
       " 'marine',\n",
       " 'opt',\n",
       " 'hand',\n",
       " 'environment',\n",
       " 'step',\n",
       " 'rest',\n",
       " 'fixed',\n",
       " 'follow',\n",
       " 'lost',\n",
       " 'sperm',\n",
       " 'external',\n",
       " 'win',\n",
       " 'attention',\n",
       " 'clock',\n",
       " 'mention',\n",
       " 'notice',\n",
       " 'dynamic',\n",
       " 'sexual',\n",
       " 'penis',\n",
       " 'wait',\n",
       " 'relief',\n",
       " 'food',\n",
       " 'worry',\n",
       " 'financial',\n",
       " 'structure',\n",
       " 'direct',\n",
       " 'figure',\n",
       " 'final',\n",
       " 'region',\n",
       " 'view',\n",
       " 'appear',\n",
       " 'bill',\n",
       " 'lower',\n",
       " 'routine',\n",
       " 'fall',\n",
       " 'biology',\n",
       " 'session',\n",
       " 'social',\n",
       " 'provided',\n",
       " 'prize',\n",
       " 'staff',\n",
       " 'cut',\n",
       " 'estate',\n",
       " 'car',\n",
       " 'completion',\n",
       " 'interrupt',\n",
       " 'monthly',\n",
       " 'electronic',\n",
       " 'difference',\n",
       " 'review',\n",
       " 'annual',\n",
       " 'da',\n",
       " 'feminist',\n",
       " 'wire',\n",
       " 'solid',\n",
       " 'wall',\n",
       " 'accelerate',\n",
       " 'separator',\n",
       " 'diet',\n",
       " 'portfolio',\n",
       " 'ways',\n",
       " 'success',\n",
       " 'documentation',\n",
       " 'earth',\n",
       " 'picture',\n",
       " 'night',\n",
       " 'plasma',\n",
       " 'classes',\n",
       " 'earning',\n",
       " 'mac',\n",
       " 'header',\n",
       " 'ribbon',\n",
       " 'terminal',\n",
       " 'remote',\n",
       " 'air',\n",
       " 'white',\n",
       " 'title',\n",
       " 'pack',\n",
       " 'purpose',\n",
       " 'agent',\n",
       " 'kit',\n",
       " 'suite',\n",
       " 'direction',\n",
       " 'learn',\n",
       " 'road',\n",
       " 'street',\n",
       " 'rework',\n",
       " 'trouble',\n",
       " 'require',\n",
       " 'appropriate',\n",
       " 'considered',\n",
       " 'brought',\n",
       " 'si',\n",
       " 'human',\n",
       " 'basic',\n",
       " 'pill',\n",
       " 'talking',\n",
       " 'leading',\n",
       " 'thinking',\n",
       " 'transfer',\n",
       " 'decision',\n",
       " 'attached',\n",
       " 'star',\n",
       " 'quick',\n",
       " 'dos',\n",
       " 'script',\n",
       " 'mark',\n",
       " 'measure',\n",
       " 'late',\n",
       " 'eric',\n",
       " 'mass',\n",
       " 'revenue',\n",
       " 'pain',\n",
       " 'target',\n",
       " 'controller',\n",
       " 'handle',\n",
       " 'ship',\n",
       " 'paragraph',\n",
       " 'union',\n",
       " 'discovery',\n",
       " 'razor',\n",
       " 'easily',\n",
       " 'defined',\n",
       " 'sending',\n",
       " 'leave',\n",
       " 'applied',\n",
       " 'commercial',\n",
       " 'familiar',\n",
       " 'correctly',\n",
       " 'topic',\n",
       " 'academic',\n",
       " 'award',\n",
       " 'mine',\n",
       " 'travel',\n",
       " 'phoenix',\n",
       " 'platform',\n",
       " 'overview',\n",
       " 'understanding',\n",
       " 'assured',\n",
       " 'compiler',\n",
       " 'symbol',\n",
       " 'approach',\n",
       " 'grounded',\n",
       " 'bring',\n",
       " 'treatment',\n",
       " 'period',\n",
       " 'chair',\n",
       " 'jeff',\n",
       " 'flow',\n",
       " 'theory',\n",
       " 'completely',\n",
       " 'io',\n",
       " 'cheap',\n",
       " 'commence',\n",
       " 'booming',\n",
       " 'party',\n",
       " 'van',\n",
       " 'al',\n",
       " 'average',\n",
       " 'medium',\n",
       " 'art',\n",
       " 'organization',\n",
       " 'performance',\n",
       " 'fund',\n",
       " 'develop',\n",
       " 'module',\n",
       " 'default',\n",
       " 'told',\n",
       " 'math',\n",
       " 'tag',\n",
       " 'peter',\n",
       " 'teaching',\n",
       " 'faster',\n",
       " 'accept',\n",
       " 'united',\n",
       " 'checked',\n",
       " 'blood',\n",
       " 'consumer',\n",
       " 'hear',\n",
       " 'extensive',\n",
       " 'double',\n",
       " 'safe',\n",
       " 'payment',\n",
       " 'guarantee',\n",
       " 'lab',\n",
       " 'quarter',\n",
       " 'acquire',\n",
       " 'dark',\n",
       " 'city',\n",
       " 'processor',\n",
       " 'hold',\n",
       " 'satellite',\n",
       " 'dip',\n",
       " 'testing',\n",
       " 'depression',\n",
       " 'york',\n",
       " 'loading',\n",
       " 'placement',\n",
       " 'loaded',\n",
       " 'action',\n",
       " 'individual',\n",
       " 'rocket',\n",
       " 'parker',\n",
       " 'crab',\n",
       " 'implementation',\n",
       " 'decided',\n",
       " 'internal',\n",
       " 'separate',\n",
       " 'status',\n",
       " 'analytical',\n",
       " 'education',\n",
       " 'yahoo',\n",
       " 'economic',\n",
       " 'lots',\n",
       " 'automatically',\n",
       " 'refund',\n",
       " 'happen',\n",
       " 'division',\n",
       " 'approval',\n",
       " 'object',\n",
       " 'ranging',\n",
       " 'grade',\n",
       " 'package',\n",
       " 'stage',\n",
       " 'live',\n",
       " 'robin',\n",
       " 'energetic',\n",
       " 'leverage',\n",
       " 'fulfill',\n",
       " 'disk',\n",
       " 'maximize',\n",
       " 'choose',\n",
       " 'sense',\n",
       " 'diversified',\n",
       " 'entire',\n",
       " 'variable',\n",
       " 'intend',\n",
       " 'os',\n",
       " 'confirmation',\n",
       " 'posted',\n",
       " 'mechanical',\n",
       " 'desire',\n",
       " 'wonderful',\n",
       " 'county',\n",
       " 'invest',\n",
       " 'diploma',\n",
       " 'game',\n",
       " 'float',\n",
       " 'contractual',\n",
       " 'greatly',\n",
       " 'bought',\n",
       " 'properly',\n",
       " 'regular',\n",
       " 'communicate',\n",
       " 'distribution',\n",
       " 'blank',\n",
       " 'tenacious',\n",
       " 'correspondence',\n",
       " 'family',\n",
       " 'woman',\n",
       " 'profitability',\n",
       " 'length',\n",
       " 'timing',\n",
       " 'intended',\n",
       " 'assistant',\n",
       " 'fire',\n",
       " 'pink',\n",
       " 'damage',\n",
       " 'wind',\n",
       " 'casino',\n",
       " 'journal',\n",
       " 'yesterday',\n",
       " 'description',\n",
       " 'rape',\n",
       " 'operator',\n",
       " 'summer',\n",
       " 'editor',\n",
       " 'conduct',\n",
       " 'shoot',\n",
       " 'optimal',\n",
       " 'involve',\n",
       " 'workshop',\n",
       " 'block',\n",
       " 'magazine',\n",
       " 'copper',\n",
       " 'officer',\n",
       " 'path',\n",
       " 'protocol',\n",
       " 'effort',\n",
       " 'lead',\n",
       " 'collected',\n",
       " 'male',\n",
       " 'button',\n",
       " 'rely',\n",
       " 'registered',\n",
       " 'ow',\n",
       " 'station',\n",
       " 'wonder',\n",
       " 'produce',\n",
       " 'author',\n",
       " 'growing',\n",
       " 'highest',\n",
       " 'slow',\n",
       " 'addition',\n",
       " 'ten',\n",
       " 'cover',\n",
       " 'weekend',\n",
       " 'stay',\n",
       " 'parallel',\n",
       " 'virus',\n",
       " 'event',\n",
       " 'historical',\n",
       " 'gain',\n",
       " 'fun',\n",
       " 'proprietary',\n",
       " 'content',\n",
       " 'charger',\n",
       " 'prospective',\n",
       " 'waiting',\n",
       " 'count',\n",
       " 'client',\n",
       " 'char',\n",
       " 'fill',\n",
       " 'monitor',\n",
       " 'ranch',\n",
       " 'edge',\n",
       " 'storage',\n",
       " 'latest',\n",
       " 'half',\n",
       " 'rush',\n",
       " 'recommend',\n",
       " 'assume',\n",
       " 'millions',\n",
       " 'scientific',\n",
       " 'movement',\n",
       " 'accurate',\n",
       " 'shaft',\n",
       " 'shop',\n",
       " 'mas',\n",
       " 'quantum',\n",
       " 'surface',\n",
       " 'young',\n",
       " 'echo',\n",
       " 'print',\n",
       " 'map',\n",
       " 'define',\n",
       " 'age',\n",
       " 'hook',\n",
       " 'plug',\n",
       " 'rising',\n",
       " 'senior',\n",
       " 'morning',\n",
       " 'returned',\n",
       " 'location',\n",
       " 'shrimp',\n",
       " 'council',\n",
       " 'till',\n",
       " 'root',\n",
       " 'muller',\n",
       " 'omega',\n",
       " 'geology',\n",
       " 'affiliate',\n",
       " 'enable',\n",
       " 'cool',\n",
       " 'logic',\n",
       " 'operate',\n",
       " 'ahead',\n",
       " 'bac',\n",
       " 'dating',\n",
       " 'agree',\n",
       " 'guide',\n",
       " 'feedback',\n",
       " 'policy',\n",
       " 'protection',\n",
       " 'previous',\n",
       " 'capital',\n",
       " 'tax',\n",
       " 'super',\n",
       " 'infrared',\n",
       " 'crayfish',\n",
       " 'heart',\n",
       " 'revolution',\n",
       " 'ne',\n",
       " 'effectively',\n",
       " 'document',\n",
       " 'featured',\n",
       " 'confidential',\n",
       " 'poor',\n",
       " 'fi',\n",
       " 'loop',\n",
       " 'criteria',\n",
       " 'lucky',\n",
       " 'quote',\n",
       " 'telephone',\n",
       " 'front',\n",
       " 'tested',\n",
       " 'female',\n",
       " 'billion',\n",
       " 'positive',\n",
       " 'actual',\n",
       " 'ted',\n",
       " 'informed',\n",
       " 'contents',\n",
       " 'literature',\n",
       " 'living',\n",
       " 'door',\n",
       " 'deep',\n",
       " 'batch',\n",
       " 'mile',\n",
       " 'learning',\n",
       " 'law',\n",
       " 'locked',\n",
       " 'wheel',\n",
       " 'complex',\n",
       " 'proving',\n",
       " 'hawk',\n",
       " 'moment',\n",
       " 'bootstrap',\n",
       " 'advancement',\n",
       " 'mission',\n",
       " 'official',\n",
       " 'rare',\n",
       " 'methodology',\n",
       " 'posting',\n",
       " 'biological']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 100\n",
    "\n",
    "VOCABULARY_100 = [x[0] for x in top_words if x[1] > k]\n",
    "\n",
    "VOCABULARY_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3ce8848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 50\n",
    "\n",
    "VOCABULARY_50 = [x[0] for x in top_words if x[1] == k]\n",
    "\n",
    "len(VOCABULARY_50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e15c1f",
   "metadata": {},
   "source": [
    "## Train Ham and Spam Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f17ebd15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengths of datasets:\n",
      "train_ham: 7523\n",
      "train_spam: 13777\n"
     ]
    }
   ],
   "source": [
    "# Split training dataset into ham and spam\n",
    "train_ham = train_set[train_set.label == 'ham']\n",
    "train_spam = train_set[train_set.label == 'spam']\n",
    "\n",
    "print(\"Lengths of datasets:\")\n",
    "print(\"train_ham:\", len(train_ham))\n",
    "print(\"train_spam:\", len(train_spam))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374e03d1",
   "metadata": {},
   "source": [
    "# Feature matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9bbc7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary_word_frequencies_in_emails(emails_list):\n",
    "    mat = []\n",
    "\n",
    "    for m in emails_list: # for every message in train spam set\n",
    "        email_words = m.split() # Split message\n",
    "\n",
    "        word_frequencies = []\n",
    "        for word in VOCABULARY: \n",
    "        # Count occurrence of each word\n",
    "            freq = email_words.count(word)\n",
    "            word_frequencies.append(freq)\n",
    "\n",
    "        mat.append(word_frequencies)\n",
    "        \n",
    "    return mat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947d45cd",
   "metadata": {},
   "source": [
    "### Ham Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f399fff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>will</th>\n",
       "      <th>width</th>\n",
       "      <th>board</th>\n",
       "      <th>company</th>\n",
       "      <th>price</th>\n",
       "      <th>gold</th>\n",
       "      <th>list</th>\n",
       "      <th>nil</th>\n",
       "      <th>time</th>\n",
       "      <th>help</th>\n",
       "      <th>...</th>\n",
       "      <th>hawk</th>\n",
       "      <th>moment</th>\n",
       "      <th>bootstrap</th>\n",
       "      <th>advancement</th>\n",
       "      <th>mission</th>\n",
       "      <th>official</th>\n",
       "      <th>rare</th>\n",
       "      <th>methodology</th>\n",
       "      <th>posting</th>\n",
       "      <th>biological</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   will  width  board  company  price  gold  list  nil  time  help  ...  hawk  \\\n",
       "0     2      0      0        0      0     0     7    0     0     1  ...     0   \n",
       "1     0      0      0        0      0     0     2    0     0     0  ...     0   \n",
       "2     0      0      0        0      0     0     0    0     0     0  ...     0   \n",
       "3     0      0      0        0      0     0     1    0     0     0  ...     0   \n",
       "4     0      0      0        0      0     0     4    0     0     0  ...     0   \n",
       "\n",
       "   moment  bootstrap  advancement  mission  official  rare  methodology  \\\n",
       "0       2          0            0        0         1     0            0   \n",
       "1       0          0            0        0         0     0            0   \n",
       "2       0          0            0        0         0     0            0   \n",
       "3       0          0            0        0         0     0            0   \n",
       "4       0          0            0        0         0     0            0   \n",
       "\n",
       "   posting  biological  \n",
       "0        0           0  \n",
       "1        0           0  \n",
       "2        0           0  \n",
       "3        0           0  \n",
       "4        0           0  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_mat = get_vocabulary_word_frequencies_in_emails(train_ham['message'])\n",
    "\n",
    "ham_feature_mat = pd.DataFrame(feature_mat, columns=VOCABULARY)\n",
    "ham_feature_mat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d58c978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save ham feature matrix\n",
    "ham_feature_mat.to_csv('feature_mat_ham_1000.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d20b985f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>will</th>\n",
       "      <th>width</th>\n",
       "      <th>board</th>\n",
       "      <th>company</th>\n",
       "      <th>price</th>\n",
       "      <th>gold</th>\n",
       "      <th>list</th>\n",
       "      <th>nil</th>\n",
       "      <th>time</th>\n",
       "      <th>help</th>\n",
       "      <th>...</th>\n",
       "      <th>hawk</th>\n",
       "      <th>moment</th>\n",
       "      <th>bootstrap</th>\n",
       "      <th>advancement</th>\n",
       "      <th>mission</th>\n",
       "      <th>official</th>\n",
       "      <th>rare</th>\n",
       "      <th>methodology</th>\n",
       "      <th>posting</th>\n",
       "      <th>biological</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   will  width  board  company  price  gold  list  nil  time  help  ...  hawk  \\\n",
       "0     2      0      0        0      0     0     7    0     0     1  ...     0   \n",
       "1     0      0      0        0      0     0     2    0     0     0  ...     0   \n",
       "2     0      0      0        0      0     0     0    0     0     0  ...     0   \n",
       "3     0      0      0        0      0     0     1    0     0     0  ...     0   \n",
       "4     0      0      0        0      0     0     4    0     0     0  ...     0   \n",
       "\n",
       "   moment  bootstrap  advancement  mission  official  rare  methodology  \\\n",
       "0       2          0            0        0         1     0            0   \n",
       "1       0          0            0        0         0     0            0   \n",
       "2       0          0            0        0         0     0            0   \n",
       "3       0          0            0        0         0     0            0   \n",
       "4       0          0            0        0         0     0            0   \n",
       "\n",
       "   posting  biological  \n",
       "0        0           0  \n",
       "1        0           0  \n",
       "2        0           0  \n",
       "3        0           0  \n",
       "4        0           0  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ham feature matrix\n",
    "ham_feature_mat = pd.read_csv('feature_mat_ham_1000.csv')\n",
    "ham_feature_mat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9e5f16",
   "metadata": {},
   "source": [
    "### Spam Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57e2ac6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>will</th>\n",
       "      <th>width</th>\n",
       "      <th>board</th>\n",
       "      <th>company</th>\n",
       "      <th>price</th>\n",
       "      <th>gold</th>\n",
       "      <th>list</th>\n",
       "      <th>nil</th>\n",
       "      <th>time</th>\n",
       "      <th>help</th>\n",
       "      <th>...</th>\n",
       "      <th>hawk</th>\n",
       "      <th>moment</th>\n",
       "      <th>bootstrap</th>\n",
       "      <th>advancement</th>\n",
       "      <th>mission</th>\n",
       "      <th>official</th>\n",
       "      <th>rare</th>\n",
       "      <th>methodology</th>\n",
       "      <th>posting</th>\n",
       "      <th>biological</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   will  width  board  company  price  gold  list  nil  time  help  ...  hawk  \\\n",
       "0     0      0      0        0      0     1     0    0     0     0  ...     0   \n",
       "1     0      0      0        0      0     0     0    0     2     0  ...     0   \n",
       "2     0      0      0        0      0     0     0    0     0     0  ...     0   \n",
       "3     1      0      0        0      0     0     0    0     0     0  ...     0   \n",
       "4     6      0      0        0      2     0     0    0     0     0  ...     0   \n",
       "\n",
       "   moment  bootstrap  advancement  mission  official  rare  methodology  \\\n",
       "0       0          0            0        0         0     0            0   \n",
       "1       0          0            0        0         0     0            0   \n",
       "2       0          0            0        0         0     0            0   \n",
       "3       0          0            0        0         0     0            0   \n",
       "4       0          0            0        0         0     0            0   \n",
       "\n",
       "   posting  biological  \n",
       "0        0           0  \n",
       "1        0           0  \n",
       "2        0           0  \n",
       "3        0           0  \n",
       "4        0           0  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_mat = get_vocabulary_word_frequencies_in_emails(train_spam['message'])\n",
    "\n",
    "spam_feature_mat = pd.DataFrame(feature_mat, columns=VOCABULARY)\n",
    "spam_feature_mat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5979368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save spam feature matrix\n",
    "spam_feature_mat.to_csv('feature_mat_spam_1000.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1dccb70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>will</th>\n",
       "      <th>width</th>\n",
       "      <th>board</th>\n",
       "      <th>company</th>\n",
       "      <th>price</th>\n",
       "      <th>gold</th>\n",
       "      <th>list</th>\n",
       "      <th>nil</th>\n",
       "      <th>time</th>\n",
       "      <th>help</th>\n",
       "      <th>...</th>\n",
       "      <th>hawk</th>\n",
       "      <th>moment</th>\n",
       "      <th>bootstrap</th>\n",
       "      <th>advancement</th>\n",
       "      <th>mission</th>\n",
       "      <th>official</th>\n",
       "      <th>rare</th>\n",
       "      <th>methodology</th>\n",
       "      <th>posting</th>\n",
       "      <th>biological</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   will  width  board  company  price  gold  list  nil  time  help  ...  hawk  \\\n",
       "0     0      0      0        0      0     1     0    0     0     0  ...     0   \n",
       "1     0      0      0        0      0     0     0    0     2     0  ...     0   \n",
       "2     0      0      0        0      0     0     0    0     0     0  ...     0   \n",
       "3     1      0      0        0      0     0     0    0     0     0  ...     0   \n",
       "4     6      0      0        0      2     0     0    0     0     0  ...     0   \n",
       "\n",
       "   moment  bootstrap  advancement  mission  official  rare  methodology  \\\n",
       "0       0          0            0        0         0     0            0   \n",
       "1       0          0            0        0         0     0            0   \n",
       "2       0          0            0        0         0     0            0   \n",
       "3       0          0            0        0         0     0            0   \n",
       "4       0          0            0        0         0     0            0   \n",
       "\n",
       "   posting  biological  \n",
       "0        0           0  \n",
       "1        0           0  \n",
       "2        0           0  \n",
       "3        0           0  \n",
       "4        0           0  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spam feature matrix\n",
    "spam_feature_mat = pd.read_csv('feature_mat_spam_1000.csv')\n",
    "spam_feature_mat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d4441d",
   "metadata": {},
   "source": [
    "# Computing prior probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f1b3f0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3531924882629108"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prior_prob_ham = len(train_ham)/len(train_set)\n",
    "prior_prob_spam = len(train_spam)/len(train_set)\n",
    "\n",
    "prior_prob_ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f116c0ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6468075117370892"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prior_prob_spam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646661e7",
   "metadata": {},
   "source": [
    "# Computing the Likelihood of each word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5dfea759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency/count vectors for each class\n",
    "word_count_ham = {}\n",
    "word_count_spam = {}\n",
    "\n",
    "for w in VOCABULARY:\n",
    "    word_count_ham[w] = ham_feature_mat[w].sum()\n",
    "\n",
    "\n",
    "for w in VOCABULARY:\n",
    "    word_count_spam[w] = spam_feature_mat[w].sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4558ac5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "269186"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_word_count_spam = 0\n",
    "\n",
    "for w in VOCABULARY:\n",
    "    total_word_count_spam += spam_feature_mat[w].sum()\n",
    "\n",
    "    \n",
    "total_word_count_spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fa65d62e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "319742"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_word_count_ham = 0\n",
    "\n",
    "for w in VOCABULARY:\n",
    "    total_word_count_ham += ham_feature_mat[w].sum()\n",
    "\n",
    "total_word_count_ham"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a7af9d",
   "metadata": {},
   "source": [
    "### Probability of a word given class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e452bdda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ld = 1\n",
    "word_total = len(VOCABULARY)\n",
    "\n",
    "def probability_given_class(w, c):\n",
    "    total_word_count_class = 0\n",
    "    if c.lower() == 'spam':\n",
    "        total_word_count_class = total_word_count_spam\n",
    "    else:\n",
    "        total_word_count_class = total_word_count_ham\n",
    "        \n",
    "        \n",
    "    word_count_in_class = 0\n",
    "    if c.lower() == 'spam':\n",
    "        word_count_in_class = word_count_spam.get(w, 0)\n",
    "    else:\n",
    "        word_count_in_class = word_count_ham.get(w, 0)\n",
    "    \n",
    "    p = (word_count_in_class + ld)/(total_word_count_class + (ld*word_total))\n",
    "    \n",
    "    return p\n",
    "\n",
    "word_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0aa8987c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02033410030491797"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probability_given_class('will', 'ham')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c87178c",
   "metadata": {},
   "source": [
    "# Classifying the emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "64c4fe18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Separated ham and spam probability computation for easier understanding :)\n",
    "def probability_ham(email):\n",
    "    \n",
    "    p = 0\n",
    "    for word in email:\n",
    "        p += math.log(probability_given_class(word, \"ham\"))\n",
    "        \n",
    "    p += math.log(prior_prob_ham)\n",
    "    \n",
    "    return p\n",
    "\n",
    "\n",
    "def probability_spam(email):\n",
    "    \n",
    "    p = 0\n",
    "    for word in email:\n",
    "        p += math.log(probability_given_class(word, \"spam\"))\n",
    "        \n",
    "    p += math.log(prior_prob_spam)\n",
    "    \n",
    "    return p\n",
    "\n",
    "\n",
    "def classify_email(email):\n",
    "    \n",
    "    p_ham = probability_ham(email)\n",
    "    p_spam = probability_spam(email)\n",
    "    \n",
    "    if p_ham > p_spam:\n",
    "        return \"ham\"\n",
    "    \n",
    "    return \"spam\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f492893",
   "metadata": {},
   "source": [
    "# Classify Emails in Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "49e5169b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parent_path = \"trec06p-cs280/data/\"\n",
    "# filepaths = test_set['filepath'].tolist() # List of filepaths for ham emails (training set)\n",
    "\n",
    "# messages = [] # List containing essential words in email message/body\n",
    "\n",
    "# for filepath in filepaths:\n",
    "#     rel_path =  parent_path + filepath\n",
    "\n",
    "#     with open(rel_path, 'rb') as file:\n",
    "#         email_file = message_from_binary_file(file)\n",
    "#         email_body = \"\"\n",
    "\n",
    "#         if email_file.is_multipart():\n",
    "#             # Take first text/plain part as email body\n",
    "#             for part in email_file.walk():\n",
    "#                 if part.get_content_type() == 'text/plain':\n",
    "#                     email_body = part.get_payload().lower()\n",
    "#                     break\n",
    "#         else:\n",
    "#             email_body = email_file.get_payload().lower()\n",
    "\n",
    "#         # Remove unnecessary elements\n",
    "#         email_body = re.sub(r'[A-Za-z0-9]*@[A-Za-z]*\\.?[A-Za-z0-9]*', \"\", email_body) # Remove email addresses\n",
    "#         email_body = re.sub(r'[0-9]+', \"\", email_body) # Remove numbers\n",
    "#         email_body = re.sub(r'[^\\'\\w\\s]', \"\", email_body) #  Remove all punctuation marks except apostrophe (')\n",
    "\n",
    "#         words_list = re.split(\"\\s\", email_body)\n",
    "\n",
    "#         while(\"\" in words_list): # Remove whitespaces\n",
    "#             words_list.remove(\"\")\n",
    "\n",
    "#         # Remove stop words\n",
    "#         essential_words_list = [word for word in words_list if word not in STOP_WORDS_LIST]\n",
    "\n",
    "#         # Use NLTK\n",
    "#         essential_words_list = [word for word in essential_words_list if word in nltk_words]\n",
    "\n",
    "#         message = \" \".join(essential_words_list)\n",
    "\n",
    "#         messages.append(message)\n",
    "\n",
    "\n",
    "# test_set.insert(2, \"message\", messages, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "92efbe0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save data to csv file (so that we don't need to run email extraction again for test set (takes 30 mins-1 hour))\n",
    "# test_set.to_csv('test_set.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f1b8b3",
   "metadata": {},
   "source": [
    "## Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "380b9536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>filepath</th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21300</td>\n",
       "      <td>../data/071/000</td>\n",
       "      <td>spam</td>\n",
       "      <td>hesitantly derive perverse satisfaction midwif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21301</td>\n",
       "      <td>../data/071/001</td>\n",
       "      <td>ham</td>\n",
       "      <td>perform experiment display will remain screen ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21302</td>\n",
       "      <td>../data/071/002</td>\n",
       "      <td>spam</td>\n",
       "      <td>best offer month time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21303</td>\n",
       "      <td>../data/071/003</td>\n",
       "      <td>spam</td>\n",
       "      <td>de ar matter ow real st te en simply ower mont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21304</td>\n",
       "      <td>../data/071/004</td>\n",
       "      <td>spam</td>\n",
       "      <td>special offer adobe video collection adobe pre...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0         filepath label  \\\n",
       "0       21300  ../data/071/000  spam   \n",
       "1       21301  ../data/071/001   ham   \n",
       "2       21302  ../data/071/002  spam   \n",
       "3       21303  ../data/071/003  spam   \n",
       "4       21304  ../data/071/004  spam   \n",
       "\n",
       "                                             message  \n",
       "0  hesitantly derive perverse satisfaction midwif...  \n",
       "1  perform experiment display will remain screen ...  \n",
       "2                              best offer month time  \n",
       "3  de ar matter ow real st te en simply ower mont...  \n",
       "4  special offer adobe video collection adobe pre...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Set\n",
    "\n",
    "test_set = pd.read_csv('test_set.csv')\n",
    "test_set = test_set.fillna(\"\")\n",
    "test_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ae2cd5",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0a56a694",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_pred = []\n",
    "\n",
    "for email in test_set['message']:\n",
    "    c = classify_email(email.split())\n",
    "    test_set_pred.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c5505c90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hesitantly derive perverse satisfaction midwif...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>perform experiment display will remain screen ...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>best offer month time</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>de ar matter ow real st te en simply ower mont...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>special offer adobe video collection adobe pre...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>message mime format bit bloodroot freedom leat...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td></td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>incorporation ad den mail tele management glad...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>choice best choice pill soft pill pill soft pi...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>subject filter hopefully blank subject odd pie...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message label\n",
       "0  hesitantly derive perverse satisfaction midwif...  spam\n",
       "1  perform experiment display will remain screen ...   ham\n",
       "2                              best offer month time  spam\n",
       "3  de ar matter ow real st te en simply ower mont...  spam\n",
       "4  special offer adobe video collection adobe pre...  spam\n",
       "5  message mime format bit bloodroot freedom leat...  spam\n",
       "6                                                     spam\n",
       "7  incorporation ad den mail tele management glad...  spam\n",
       "8  choice best choice pill soft pill pill soft pi...  spam\n",
       "9  subject filter hopefully blank subject odd pie...   ham"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set[['message', 'label']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b8f8820c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spam', 'ham', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'spam', 'ham']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "24fd0232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16522, 4)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "83eccb22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16522"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_set_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fb4b1670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'spam': 11087, 'ham': 5435})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(test_set_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4471791",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5335698d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.939111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.929916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.931877</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Score\n",
       "Accuracy   0.939111\n",
       "Precision  0.929916\n",
       "Recall     0.931877"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, precision_recall_fscore_support\n",
    "\n",
    "precision = precision_score(test_set['label'].tolist(), test_set_pred, average='macro')\n",
    "recall = recall_score(test_set['label'].tolist(), test_set_pred, average='macro')\n",
    "accuracy = accuracy_score(test_set['label'].tolist(), test_set_pred, normalize=True, sample_weight=None)\n",
    "\n",
    "eval_results = pd.DataFrame({'Accuracy': accuracy, 'Precision': precision, 'Recall': recall}, index=['Score']).transpose()\n",
    "eval_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
